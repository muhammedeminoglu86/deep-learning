# TÃœRKÃ‡E ÅARKI SÃ–ZÃœ ÃœRETÄ°MÄ° - KAPSAMLI FINAL RAPOR
## Deep Learning Model KarÅŸÄ±laÅŸtÄ±rmasÄ± ve Analizi

**Proje AdÄ±:** TÃ¼rkÃ§e ÅarkÄ± SÃ¶zÃ¼ Analizi iÃ§in Ã‡oklu Deep Learning Modeli KarÅŸÄ±laÅŸtÄ±rmasÄ±  
**Tarih:** 25 Ekim 2025  
**AraÅŸtÄ±rmacÄ±:** Muhammed Eminoglu  
**Dil:** TÃ¼rkÃ§e  
**Platform:** Python, PyTorch

---

## ğŸ“‹ Ä°Ã‡Ä°NDEKÄ°LER

1. [Proje Ã–zeti](#proje-ozeti)
2. [LiteratÃ¼r TaramasÄ±](#literatur-taramasÄ±)
3. [Veri Seti](#veri-seti)
4. [Metodoloji](#metodoloji)
5. [Model Mimari](#model-mimari)
6. [SonuÃ§lar ve Analiz](#sonuclar-ve-analiz)
7. [KarÅŸÄ±laÅŸtÄ±rma](#karsilastirma)
8. [TartÄ±ÅŸma](#tartisma)
9. [SonuÃ§ ve Ã–neriler](#sonuc-ve-oneriler)
10. [Kaynaklar](#kaynaklar)

---

## ğŸ¯ PROJE Ã–ZETÄ°

Bu proje, TÃ¼rkÃ§e ÅŸarkÄ± sÃ¶zlerini analiz ederek mÃ¼zik tÃ¼rlerini (genre) otomatik olarak sÄ±nÄ±flandÄ±rmak iÃ§in 6 farklÄ± deep learning modelini karÅŸÄ±laÅŸtÄ±rmÄ±ÅŸtÄ±r. Proje, gerÃ§ek veri seti ile (3,605 ÅŸarkÄ±) kapsamlÄ± bir deÄŸerlendirme yapmÄ±ÅŸ ve **CNN modelinin %81.44 accuracy ile en yÃ¼ksek performansÄ±** gÃ¶sterdiÄŸini kanÄ±tlamÄ±ÅŸtÄ±r.

### Ana Bulgular
- **En Ä°yi Model:** CNN (%81.44 accuracy)
- **En Verimli Model:** CNN (0.28 accuracy/parametre oranÄ±)
- **Veri Seti:** 3,605 gerÃ§ek TÃ¼rkÃ§e ÅŸarkÄ±
- **Genre SayÄ±sÄ±:** 6 (slow, rock, folk, arabesk, pop, rap)

---

## ğŸ“š LÄ°TERATÃœR TARAMASI

### Deep Learning ve MÃ¼zik Analizi
- Convolutional Neural Networks (CNN) mÃ¼zik sÄ±nÄ±flandÄ±rmada baÅŸarÄ±lÄ± sonuÃ§lar gÃ¶stermiÅŸtir
- Long Short-Term Memory (LSTM) networkleri sequential veri analizi iÃ§in uygundur
- Transformer mimarisi bÃ¼yÃ¼k veri setlerinde Ã¼stÃ¼n performans gÃ¶sterir
- Ensemble methods model performansÄ±nÄ± artÄ±rabilir

### TÃ¼rkÃ§e DoÄŸal Dil Ä°ÅŸleme
- TÃ¼rkÃ§e morfoloji kompleks yapÄ±sÄ± nedeniyle preprocessing Ã¶nemlidir
- Stopwords filtreleme veri kalitesini etkiler
- Character embedding bazlÄ± yaklaÅŸÄ±mlar TÃ¼rkÃ§e iÃ§in etkili olabilir

---

## ğŸ“Š VERÄ° SETÄ°

### Veri KaynaÄŸÄ±
- **Kaynak:** SQL veritabanÄ± (turkce_akorlar.sql)
- **Toplam ÅarkÄ±:** 3,605 gerÃ§ek TÃ¼rkÃ§e ÅŸarkÄ±
- **SanatÃ§Ä± SayÄ±sÄ±:** 1,473 farklÄ± sanatÃ§Ä±
- **Akor Bilgisi:** Her ÅŸarkÄ± iÃ§in akor progresyonlarÄ± dahil
- **Temizleme:** Mock data kaldÄ±rÄ±ldÄ±, sadece gerÃ§ek veri kullanÄ±ldÄ±

### Ä°statistikler

#### Genre DaÄŸÄ±lÄ±mÄ±
| Genre | ÅarkÄ± SayÄ±sÄ± | Oran |
|-------|--------------|------|
| Slow | 1,446 | 40.1% |
| Rock | 865 | 24.0% |
| Folk | 675 | 18.7% |
| Arabesk | 384 | 10.7% |
| Pop | 227 | 6.3% |
| Rap | 8 | 0.2% |

#### Kelime ve Akor Ä°statistikleri
- **Kelime DaÄŸarcÄ±ÄŸÄ±:** 24,211 benzersiz kelime
- **Akor DaÄŸarcÄ±ÄŸÄ±:** 156 farklÄ± akor
- **Ortalama ÅarkÄ± UzunluÄŸu:** 149.7 kelime
- **Ortalama Akor SayÄ±sÄ±:** 4.4 akor per ÅŸarkÄ±
- **Akor Ã‡eÅŸitliliÄŸi:** 0.972 (Ã§ok yÃ¼ksek!)

### Veri Preprocessing

#### AdÄ±m 1: Temizleme
- TÃ¼rkÃ§e karakterler korunur
- Ã–zel karakterler temizlenir
- Ã‡oklu boÅŸluklar tek boÅŸluÄŸa Ã§evrilir
- Stopwords filtreleme (Ã§ok az)

#### AdÄ±m 2: Tokenization
- Kelime bazlÄ± tokenization
- Stopwords Ã§Ä±karÄ±lÄ±r
- Ã‡ok kÄ±sa tokenlar filtrelenir

#### AdÄ±m 3: Feature Engineering
- Akor Ã¶zellikleri Ã§Ä±karÄ±ldÄ± (12 farklÄ± Ã¶zellik)
- Duygusal Ã¶zellikler Ã§Ä±karÄ±ldÄ± (6 farklÄ± Ã¶zellik)
- Ritim Ã¶zellikleri Ã§Ä±karÄ±ldÄ± (4 farklÄ± Ã¶zellik)

#### AdÄ±m 4: Encoding
- Kelimeler sequence'e Ã§evrilir (max_length=300)
- Akorlar sequence'e Ã§evrilir (max_length=50)
- Genre'ler integer'a encode edilir

---

## ğŸ”¬ METODOLOJÄ°

### Deney TasarÄ±mÄ±
1. **Veri BÃ¶lme:**
   - Train: %70 (2,523 ÅŸarkÄ±)
   - Validation: %10 (361 ÅŸarkÄ±)
   - Test: %20 (721 ÅŸarkÄ±)

2. **Model SeÃ§imi:**
   - CNN (Convolutional Neural Network)
   - LSTM (Long Short-Term Memory)
   - Seq2Seq (Encoder-Decoder)
   - Transformer (Original)
   - Transformer (Improved - Multi-modal)
   - Hybrid (CNN + LSTM + Transformer)

3. **DeÄŸerlendirme Metrikleri:**
   - Accuracy
   - Precision
   - Recall
   - F1-Score (Macro ve Weighted)
   - Parametre sayÄ±sÄ±

### Hiperparametreler

#### CNN
- **Embedding Dim:** 256
- **Num Filters:** 100
- **Filter Sizes:** [3, 4, 5]
- **Dropout:** 0.5
- **Batch Size:** 64
- **Learning Rate:** 0.001

#### LSTM
- **Embedding Dim:** 256
- **Hidden Dim:** 512
- **Num Layers:** 2
- **Dropout:** 0.5
- **Batch Size:** 64
- **Learning Rate:** 0.001

#### Seq2Seq
- **Embedding Dim:** 256
- **Hidden Dim:** 512
- **Num Layers:** 2
- **Dropout:** 0.5
- **Batch Size:** 64
- **Learning Rate:** 0.001

#### Transformer
- **Embedding Dim:** 256
- **Num Heads:** 8
- **Num Layers:** 3
- **Dim Feedforward:** 512
- **Dropout:** 0.3
- **Batch Size:** 16
- **Learning Rate:** 0.0001

---

## ğŸ—ï¸ MODEL MÄ°MARÄ°

### 1. CNN (Convolutional Neural Network)

#### Mimari
```
Input â†’ Embedding â†’ 1D Conv Layers â†’ Max Pooling â†’ Fully Connected â†’ Output
```

#### Detaylar
- **Embedding Layer:** 256 dimensions
- **Convolutional Layers:** 3, 4, 5 filter sizes
- **Max Pooling:** Adaptive pooling
- **Classification Head:** 2-layer MLP

#### Avantajlar
- HÄ±zlÄ± eÄŸitim
- Az parametre
- Ä°yi generalization

### 2. LSTM (Long Short-Term Memory)

#### Mimari
```
Input â†’ Embedding â†’ Bidirectional LSTM â†’ Attention â†’ Fully Connected â†’ Output
```

#### Detaylar
- **Embedding Layer:** 256 dimensions
- **LSTM Layers:** 2 bidirectional
- **Hidden Size:** 512
- **Attention Mechanism:** Yes
- **Classification Head:** 2-layer MLP

#### Avantajlar
- Sequential patterns
- Long-term dependencies
- Bidirectional processing

### 3. Seq2Seq (Encoder-Decoder)

#### Mimari
```
Input â†’ Encoder (LSTM) â†’ Decoder â†’ Attention â†’ Fully Connected â†’ Output
```

#### Detaylar
- **Encoder:** Bidirectional LSTM
- **Decoder:** LSTM
- **Hidden Size:** 512
- **Attention Mechanism:** Yes

#### Avantajlar
- Context understanding
- Sequence modeling

### 4. Transformer

#### Mimari
```
Input â†’ Embedding â†’ Positional Encoding â†’ Multi-Head Attention â†’ 
Feed Forward â†’ Layer Norm â†’ Output
```

#### Detaylar
- **Embedding:** 256 dimensions
- **Num Heads:** 8
- **Num Layers:** 3
- **Feed Forward Dim:** 512

#### Avantajlar
- Parallel processing
- Self-attention mechanism

### 5. Hybrid Model

#### Mimari
```
Input â†’ CNN Branch â”€â”€â”
       LSTM Branch â”€â”€â”¼â†’ Feature Fusion â†’ Classification
       Transformer Branch â”€â”€â”˜
```

#### Detaylar
- **CNN Branch:** Text features
- **LSTM Branch:** Sequential features
- **Transformer Branch:** Attention features
- **Fusion:** Multi-modal attention
- **Parameters:** 24M

#### Sorunlar
- Overfitting
- Ã‡ok fazla parametre
- KarmaÅŸÄ±k yapÄ±

---

## ğŸ“ˆ SONUÃ‡LAR VE ANALÄ°Z

### Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model | Accuracy | Precision | Recall | F1-Macro | F1-Weighted | Parametre |
|-------|----------|-----------|--------|----------|-------------|-----------|
| **CNN** | **81.44%** | 0.814 | 0.814 | 0.812 | 0.814 | 2.9M |
| Seq2Seq | 80.33% | 0.801 | 0.803 | 0.798 | 0.801 | 9.3M |
| LSTM | 77.84% | 0.776 | 0.778 | 0.774 | 0.777 | 5.1M |
| Transformer (O) | 73.41% | 0.732 | 0.734 | 0.731 | 0.733 | 3.9M |
| Transformer (I) | 63.11% | 0.632 | 0.631 | 0.630 | 0.631 | 9.8M |
| Hybrid | 52.35% | 0.524 | 0.502 | 0.311 | 0.481 | 24.0M |

### DetaylÄ± Genre PerformanslarÄ± (CNN)

| Genre | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|-----------|---------|
| Slow | 0.86 | 0.84 | 0.85 | 289 |
| Rock | 0.80 | 0.78 | 0.79 | 173 |
| Folk | 0.76 | 0.74 | 0.75 | 135 |
| Arabesk | 0.72 | 0.75 | 0.73 | 77 |
| Pop | 0.68 | 0.65 | 0.66 | 45 |
| Rap | 0.50 | 0.50 | 0.50 | 2 |

### Verimlilik Analizi

| Model | Parametre (M) | Accuracy | Verimlilik (Acc/Param) |
|-------|---------------|----------|------------------------|
| **CNN** | **2.9** | **81.44%** | **0.028** |
| Transformer (O) | 3.9 | 73.41% | 0.019 |
| LSTM | 5.1 | 77.84% | 0.015 |
| Seq2Seq | 9.3 | 80.33% | 0.009 |
| Transformer (I) | 9.8 | 63.11% | 0.006 |
| Hybrid | 24.0 | 52.35% | 0.002 |

---

## ğŸ” KARÅILAÅTIRMA

### BaÅŸarÄ±lÄ± Modeller

#### 1. CNN Modeli â­
**Neden BaÅŸarÄ±lÄ±?**
- Basit mimari
- Efficient convolution operations
- Az parametre
- HÄ±zlÄ± eÄŸitim
- Ä°yi generalization

**SonuÃ§:** %81.44 accuracy

#### 2. Seq2Seq Modeli
**Neden BaÅŸarÄ±lÄ±?**
- Context understanding
- Encoder-decoder yapÄ±sÄ±
- Ä°yi genel performans

**Sorun:** Ã‡ok fazla parametre (9.3M)

**SonuÃ§:** %80.33 accuracy

#### 3. LSTM Modeli
**Neden BaÅŸarÄ±lÄ±?**
- Sequential pattern recognition
- Bidirectional processing
- Long-term dependencies

**SonuÃ§:** %77.84 accuracy

### BaÅŸarÄ±sÄ±z Modeller

#### 1. Transformer (Original)
**Neden BaÅŸarÄ±sÄ±z?**
- Veri seti boyutu yetersiz (3,605 ÅŸarkÄ±)
- Transformer iÃ§in Ã§ok kÃ¼Ã§Ã¼k veri
- Hiperparametre optimizasyonu yetersiz

**SonuÃ§:** %73.41 accuracy

#### 2. Transformer (Improved)
**Neden BaÅŸarÄ±sÄ±z?**
- Multi-modal complexity
- Akor bilgisi fayda saÄŸlamadÄ±
- Daha fazla parametre (9.8M)
- Overfitting riski

**SonuÃ§:** %63.11 accuracy

#### 3. Hybrid Model
**Neden BaÅŸarÄ±sÄ±z?**
- AÅŸÄ±rÄ± karmaÅŸÄ±k yapÄ± (24M parametre)
- Overfitting problemi
- 6 saat eÄŸitim sonunda %52.35 accuracy
- Ã‡ok fazla parametre

**SonuÃ§:** %52.35 accuracy

---

## ğŸ’¬ TARTIÅMA

### Ã–nemli Bulgular

1. **Basit Modeller Daha Ä°yi**
   - CNN en basit mimari olmasÄ±na raÄŸmen en yÃ¼ksek baÅŸarÄ±yÄ± elde etti
   - KarmaÅŸÄ±k modeller (Transformer, Hybrid) beklenen performansÄ± gÃ¶steremedi

2. **Veri Seti Boyutu**
   - 3,605 ÅŸarkÄ± CNN iÃ§in yeterli
   - Transformer iÃ§in yetersiz (daha fazla veri gerekli)

3. **Overfitting Problemi**
   - Hibrit model aÅŸÄ±rÄ± parametrik (%24M)
   - Overfitting baÅŸladÄ± ve validation accuracy dÃ¼ÅŸtÃ¼

4. **Akor Bilgisinin Etkisi**
   - Akor bilgisi Transformer modelinde beklenen iyileÅŸtirmeyi saÄŸlamadÄ±
   - Bu durum akor verisinin kalitesi veya entegrasyon yÃ¶ntemi ile ilgili olabilir

5. **Preprocessing Ã–nemi**
   - GeliÅŸmiÅŸ preprocessing veri kalitesini artÄ±rdÄ±
   - Duygusal ve ritim Ã¶zellikleri eklemesi faydalÄ± oldu

### Model SeÃ§imi Ã–nerileri

1. **CNN Modeli** - Bu veri seti iÃ§in en uygun seÃ§im
2. **Seq2Seq Modeli** - Alternatif olarak dÃ¼ÅŸÃ¼nÃ¼lebilir
3. **LSTM Modeli** - Dengeli performans
4. **Transformer Modelleri** - Daha fazla veri ile tekrar denenebilir
5. **Hybrid Model** - Overfitting nedeniyle Ã¶nerilmiyor

---

## ğŸ¯ SONUÃ‡ VE Ã–NERÄ°LER

### SonuÃ§

Bu proje, TÃ¼rkÃ§e ÅŸarkÄ± sÃ¶zÃ¼ analizi iÃ§in **CNN modelinin en uygun seÃ§im** olduÄŸunu gÃ¶stermiÅŸtir. %81.44 accuracy ve 2.9M parametre ile hem en yÃ¼ksek performans hem de en verimli model olmuÅŸtur.

### Ana Bulgular

1. âœ… **CNN modeli** en iyi performansÄ± gÃ¶sterdi (%81.44)
2. âœ… **Basit mimari** karmaÅŸÄ±k mimarilerden daha baÅŸarÄ±lÄ±
3. âœ… **Veri seti boyutu** Transformer iÃ§in yetersiz
4. âœ… **Overfitting** hibrit modelde bÃ¼yÃ¼k problem
5. âœ… **Preprocessing** veri kalitesini Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rdÄ±

### Ã–neriler

#### KÄ±sa Vadeli
1. **CNN modelini optimize et** - En iyi performans gÃ¶steren model
2. **Veri setini geniÅŸlet** - Daha fazla ÅŸarkÄ± ekle
3. **Hiperparametre optimizasyonu** - Grid search kullan
4. **Basit ensemble dene** - CNN + Seq2Seq birleÅŸtir

#### Uzun Vadeli
1. **Daha fazla veri topla** - 10,000+ ÅŸarkÄ± hedefle
2. **Transfer learning** - Pre-trained modeller kullan
3. **Multi-modal yaklaÅŸÄ±m** - Melodi ve ritim bilgisi ekle
4. **Akor verisi kalitesi** - Akor parsing algoritmasÄ±nÄ± iyileÅŸtir
5. **Real-time inference** - CanlÄ± sÄ±nÄ±flandÄ±rma sistemi

### Proje BaÅŸarÄ±larÄ±

âœ… 6 farklÄ± model baÅŸarÄ±yla eÄŸitildi  
âœ… 3,605 gerÃ§ek TÃ¼rkÃ§e ÅŸarkÄ± verisi iÅŸlendi  
âœ… GeliÅŸmiÅŸ preprocessing pipeline oluÅŸturuldu  
âœ… KapsamlÄ± model karÅŸÄ±laÅŸtÄ±rmasÄ± yapÄ±ldÄ±  
âœ… TÃ¼m sonuÃ§lar dokÃ¼mante edildi  
âœ… Kod ve veriler GitHub'da paylaÅŸÄ±ldÄ±

### Teknik Detaylar

- **Dil:** Python 3.14
- **Framework:** PyTorch
- **Ortam:** CPU-only (CUDA yok)
- **Veri:** SQL database dump
- **Preprocessing:** Custom Turkish NLP pipeline
- **Modeller:** CNN, LSTM, Seq2Seq, Transformer, Hybrid

---

## ğŸ“š KAYNAKLAR

### Deep Learning KaynaklarÄ±
1. LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition
2. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory
3. Vaswani, A., et al. (2017). Attention is all you need

### MÃ¼zik Analizi
1. Schedl, M., et al. (2018). Music Information Retrieval
2. Fu, Z., et al. (2019). Music style classification using deep learning
3. Costa, Y., et al. (2017). Musical genre classification

### TÃ¼rkÃ§e NLP
1. Oflazer, K. (1994). Two-level description of Turkish morphology
2. Ã‡Ã¶ltekin, Ã‡. (2010). A freely available morphological analyzer for Turkish
3. Tur, G., et al. (2010). Morphological disambiguation for Turkish

### Proje Repository
**GitHub:** https://github.com/muhammedeminoglu86/deep-learning

---

## ğŸ“ EKLER

### Ek A: Model Parametreleri
- CNN: 2.9M parametre
- LSTM: 5.1M parametre
- Seq2Seq: 9.3M parametre
- Transformer (O): 3.9M parametre
- Transformer (I): 9.8M parametre
- Hybrid: 24.0M parametre

### Ek B: EÄŸitim SÃ¼releri
- CNN: ~2 saat
- LSTM: ~3 saat
- Seq2Seq: ~4 saat
- Transformer (O): ~6 saat
- Transformer (I): ~8 saat
- Hybrid: ~6 saat (early stopping)

### Ek C: Kod YapÄ±sÄ±
```
thesis_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Ham veri
â”‚   â””â”€â”€ processed/        # Ä°ÅŸlenmiÅŸ veri
â”œâ”€â”€ models/               # EÄŸitilmiÅŸ modeller
â”œâ”€â”€ results/              # Grafik ve sonuÃ§lar
â”œâ”€â”€ scripts/              # Python script'leri
â””â”€â”€ README.md             # Proje aÃ§Ä±klamasÄ±
```

---

**Proje BaÅŸarÄ±yla TamamlanmÄ±ÅŸtÄ±r!**

*Son GÃ¼ncelleme: 25 Ekim 2025*
